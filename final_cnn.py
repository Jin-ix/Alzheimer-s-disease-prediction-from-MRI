# -*- coding: utf-8 -*-
"""Final Cnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OZNyXgZeUfLxE06VY6bH6vwxrp97LqS0
"""

!kaggle kernels output yashborude/alzheimer-classification-2-cnn-svm-hybrid -p /path/to/dest

from google.colab import drive
import os

# Check if the directory is empty, if not, clear it or choose a different mount point
if os.listdir('/content/drive'): # Use Python's os module to check directory contents
    print("Directory is not empty")
else:
    print("Directory is empty")

# Mount Google Drive
drive.mount('/content/drive')

# After mounting, you can navigate to your Google Drive files like this:
# !ls /content/drive/MyDrive

!https://www.kaggle.com/code/yashborude/alzheimer-classification-2-cnn-svm-hybrid?scriptVersionId=180641734&cellId=1

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

import numpy as np
import pandas as pd
import os
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

import numpy as np
import pandas as pd
import os
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import shutil # Import the shutil module

# Original dataset directory
original_dataset_dir = '/content/drive/MyDrive/MRI/Dataset'

# Create new directories
base_dir = '/content/drive/MyDrive/MRI/Dataset'
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')

# Create directories for each class in train and validation directories
os.makedirs(train_dir, exist_ok=True)
os.makedirs(validation_dir, exist_ok=True)
for class_name in ['Mild_Demented', 'Moderate_Demented', 'Non_Demented', 'Very_Mild_Demented']:
    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)
    os.makedirs(os.path.join(validation_dir, class_name), exist_ok=True)

# Split the data
for class_name in ['Mild_Demented', 'Moderate_Demented', 'Non_Demented', 'Very_Mild_Demented']:
    class_dir = os.path.join(original_dataset_dir, class_name)

    # Check if the directory exists and contains files
    if os.path.exists(class_dir) and os.listdir(class_dir):
        images = os.listdir(class_dir)
        train_images, val_images = train_test_split(images, test_size=0.2, random_state=42)  # 80-20 split

        for image in train_images:
            src = os.path.join(class_dir, image)
            dst = os.path.join(train_dir, class_name, image)
            shutil.copyfile(src, dst)

        for image in val_images:
            src = os.path.join(class_dir, image)
            dst = os.path.join(validation_dir, class_name, image)
            shutil.copyfile(src, dst)
    else:
        print(f"Warning: Directory '{class_dir}' is either empty or does not exist.")

# Data augmentation and normalization for training
train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

# Only rescaling for validation
validation_datagen = ImageDataGenerator(rescale=1./255)

# Flow data from directories
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(128, 128),  # Resize images to 128x128
    batch_size=32,
    class_mode='categorical'
)

validation_generator = validation_datagen.flow_from_directory(
    validation_dir,
    target_size=(128, 128),
    batch_size=32,
    class_mode='categorical'
)

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),
    MaxPooling2D(pool_size=(2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(4, activation='softmax')  # 4 classes for Alzheimer's stages
])

model.summary()

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),
    MaxPooling2D(pool_size=(2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(4, activation='softmax')  # 4 classes for Alzheimer's stages
])

model.summary()

model.compile(
    optimizer=Adam(),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    validation_steps=validation_generator.samples // validation_generator.batch_size,
    validation_data=validation_generator,
    epochs=41  # Adjust the number of epochs as needed
)

model.compile(
    optimizer=Adam(),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    validation_steps=validation_generator.samples // validation_generator.batch_size,
    validation_data=validation_generator,
    epochs=5  # Adjust the number of epochs as needed
)

model.compile(
    optimizer=Adam(),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    validation_steps=validation_generator.samples // validation_generator.batch_size,
    validation_data=validation_generator,
    epochs=5  # Adjust the number of epochs as needed
)

loss, accuracy = model.evaluate(validation_generator)
print(f'Validation Accuracy: {accuracy * 100:.2f}%')

loss, accuracy = model.evaluate(validation_generator)
print(f'Validation Accuracy: {accuracy * 100:.2f}%')

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.legend(loc='lower right')
plt.show()

!pip install scikit-learn
import sklearn.metrics
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

# Evaluate the model
loss, accuracy = model.evaluate(validation_generator)
print(f'Validation Accuracy: {accuracy * 100:.2f}%')

# Predict the labels for validation data
Y_pred = model.predict(validation_generator)
y_pred = np.argmax(Y_pred, axis=1)

# Get the true labels
y_true = validation_generator.classes

# Print classification report
print('Classification Report')
target_names = ['Mild_Demented', 'Moderate_Demented', 'Non_Demented', 'Very_Mild_Demented']
print(classification_report(y_true, y_pred, target_names=target_names)) # Call the function after importing

# Compute confusion matrix
cm = confusion_matrix(y_true, y_pred)
print('Confusion Matrix')
print(cm)

# Assuming you have a base directory and validation directory
base_dir = '/content/drive/MyDrive/MRI/Dataset'
validation_dir = os.path.join(base_dir, 'validation')

# Print the files in the validation directory to verify it's correct
!ls "{validation_dir}"

# Data generator for validation data (no changes needed here)
validation_datagen = ImageDataGenerator(rescale=1./255)

# Flow data from validation directory (no changes needed here)
validation_generator = validation_datagen.flow_from_directory(
    validation_dir,
    target_size=(128, 128),  # Resize images
    batch_size=32,
    class_mode='categorical',
    shuffle=False  # Important: Do not shuffle for correct labels alignment
)

# Evaluate the model on validation data (no changes needed here)
loss, accuracy = model.evaluate(validation_generator)
print(f'Validation Accuracy: {accuracy * 100:.2f}%')

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image

# Function to load and preprocess image
def load_and_preprocess_image(img_path):
    img = image.load_img(img_path, target_size=(128, 128))  # Load the image and resize
    img_array = image.img_to_array(img)  # Convert to array
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
    img_array = img_array / 255.0  # Normalize to [0, 1]
    return img_array, img

# Function to predict the class of the MRI image
def predict_mri_image(img_path, model):
    img_array, img = load_and_preprocess_image(img_path)
    prediction = model.predict(img_array)
    predicted_class = np.argmax(prediction, axis=1)[0]

    # Class labels
    class_labels = ['Mild_Demented', 'Moderate_Demented', 'Non_Demented', 'Very_Mild_Demented']

    print(f'Predicted class: {class_labels[predicted_class]}')
    print(f'Class probabilities: {prediction}')

    # Display the image
    plt.imshow(img)
    plt.title(f'Predicted: {class_labels[predicted_class]}')
    plt.axis('off')
    plt.show()

# Example usage
new_image_path = '/content/mild_4.jpg'  # Replace with the path to your image
predict_mri_image(new_image_path, model)

# Save the entire model as a SavedModel.
model.save('my_model.h5')

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing import image
from google.colab import files
from PIL import Image
import matplotlib.pyplot as plt

model1 = tf.keras.models.load_model('my_model.h5')

uploaded = files.upload()

def load_and_preprocess_image(img_path):
    img = tf.keras.utils.load_img(img_path, target_size=(128, 128))
    img_array = tf.keras.utils.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
    img_array = img_array / 255.0  # Rescale to match training data
    return img_array

for fn in uploaded.keys():
    # Display the image
    img = Image.open(fn)
    plt.imshow(img)
    plt.axis('off')
    plt.show()

    # Preprocess the image and make prediction
    img_array = load_and_preprocess_image(fn)
    prediction = model.predict(img_array)
    predicted_class = np.argmax(prediction, axis=1)[0]

    # Class labels
    class_labels = ['Mild_Demented', 'Moderate_Demented', 'Non_Demented', 'Very_Mild_Demented']

    # Print the prediction
    print(f'Predicted class: {class_labels[predicted_class]}')
    print(f'Class probabilities: {prediction}')